- mini batch size: ...process as many images at a time as our graphics card allows. This is a case of trial and error to find the max batch size - the largest size that doesn't give an out of memory error. https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson2.ipynb
- weight decay from regularization: `1 - eta * lambda / n` http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization
- There is an additional twist due to how log loss is calculated--log loss rewards predictions that are confident and correct (p=.9999,label=1), but it punishes predictions that are confident and wrong far more (p=.0001,label=1). See visualization below. So to play it safe, we use a sneaky trick to round down our edge predictions. Swap all ones with .95 and all zeros with .05 `isdog = isdog.clip(min=0.05, max=0.95)` https://github.com/fastai/courses/blob/master/deeplearning1/nbs/dogs_cats_redux.ipynb
- A general rule of thumb to keep in mind is that most of our computation time lies in the convolutional layers, while our memory overhead lies in our dense layers. http://wiki.fast.ai/index.php/Lesson_3_Notes
